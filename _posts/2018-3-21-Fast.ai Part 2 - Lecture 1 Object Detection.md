---
title: Fast.ai Part 2 — Lecture 1 Object Detection
---
![](\assets\images\objectdetection.png?raw=true)

After David Uminsky, who is the Director of Data Institute, finished his welcome speech and called over Jeremy to get started with the course, Jeremy started the first class of Deep Learning Part 2 by thanking David, for making a course like this possible and available for many across the world. Though there is a lot to talk about this course and surely there is a need to spread the word about it so that many more people will get access to this course, I will spare that for a very-near-future post and for now I would like to thank David, Jeremy, Rachel and everyone else who made this course available for people from across the world.

I finished taking the first lecture of Fast.ai Part 2 course on Deep Learning a few hours ago and I want to share what was taught and learnt from it through this post. This is an unofficial lecture note. If you are a Fast.ai student already, feel free to go through it and let me know what else could be added. Otherwise, make sure you go [here](http://course.fast.ai/) and start taking the amazing lectures. Jeremy gave a quick review of few of the key takeaways from Part 1 of the course. He talked about how Deep Learning is so much about differentiable functions and gave reference to Yann Lecun’s statement, “ Deep Learning est mort. Vive Differentiable Programming!”(Click [here](https://www.facebook.com/yann.lecun/posts/10155003011462143) for the original post).

![](\assets\images\review.png?raw=true)

Transfer Learning was present all throughout Part 1. We usually start with random weights and try to learn them(find their optimal values)so that the loss gets reduced. In the case of having weights that are already learnt(even if they are not totally relevant to the task at hand), it’s always good to start with the learned weights(pre-trained models). If there is a model A that has learnt to perform a task U, even if it is only remotely close to another task V, the model can still be taken and fine-tuned by re-learning the last layers. This way the learning is quicker. CNN architecture, over-fitting and embedding were few other main concepts that were discussed in Part 1. Adding more data, data augmentation, batch normalisation, regularisation using weight decay and drop-out are ways to avoid over-fitting.

One of the best parts of Fast.ai is the tips and suggestions you receive from the class. Don’t think too much about model over-fitting initially. It’s something that should be worried about when you want to optimize the model, not when you are creating it. There will be flaws in the code that you come across and that’s not unusual. So when something doesn’t work as expected, remember this as one of the possibilities(may not always be the case though!).Whether it’s Fast.ai or any other forum, don’t be afraid/shy to ask questions. But before you do, make sure you give it a try.

> Jupyter notebooks are interactive and easy to run. So take the notebook that has the code for each lecture, start from the first cell and keep executing one after the other and see how things work. Copy & Paste whenever you need to. When you face some issues with the same code that worked in the lecture, run to the forum and shoot the questions!!

Above are some of the must DON’Ts. Simply going through each cell of the code might help you see the results of what each cell does. But it doesn’t help in getting better at coding the model or knowing the approach to solving even a slightly different task. For a much better approach, check [this](http://wiki.fast.ai/index.php/How_to_use_the_Provided_Notebooks) wiki post. It’s a very good practice to read papers, understand them, try and explain it in plain language for others through blogs and also try and implement them. This also makes one realise how, easy to understand things are made complicated in an attempt to gather theoretical knowledge. I laughed when Jeremy was saying all this. If you have taken Part 1 already, you must be aware of such harmless, fun pokes that Jeremy makes once in a while. I call them The Jeremy Jokes!

Deep Learning models usually consume a lot of data and need more than decent computational power. Typical CPUs can’t afford such computational demand. To run such models on large datasets, it’s better to go with cloud computing services or to build your own GPU box. . The below picture can be helpful with setting up your own GPU box:

![](\assets\images\buildgpubox.png?raw=true)

It’s time for Object Detection! I guess one need not explain what it is. But still for the note-sake, given an image with one or more objects, object detection is the process of locating, classifying and labelling objects of different classes/categories in the image. It is obvious that it is close to Computer Vision. To perform object detection, we chose the Pascal Visual Object Classes dataset. The Pascal VOC Project ran challenges for several years(2005–2012)evaluating performance on object class recognition. Each year’s datasets are available to download. In the lecture notebook, we used the data that was derived from 2007 challange. Besides the images, other important part of this dataset is the annotations which contain the actual information about what all objects are present in each image,the classes of each object and the bounding box . Since this annotation information is in .xml format in the original dataset, we used the derived dataset which has the same data in .json format and is easier to use. More details about the dataset and project can be found [here](http://host.robots.ox.ac.uk/pascal/VOC/index.html).

