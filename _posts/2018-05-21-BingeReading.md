---
title: Google Smart Compose, Machine Bias, Racist AI - Summarising One Night of Binge Reading from Blogs
hidden: true
---
<p align="center"><img src="/assets/images/bingereading.png"/></p>
<p align="center"><a href="https://www.forbesmiddleeast.com/en/artificial-intelligence-to-contribute-320-billion-to-the-middle-east-by-2030/">Pic Credit</a></p>  

After Sundar Pichai took the stage and started Google I/O 2018's keynote, I started to take a note of interesting things that were being announced and demoed. There were some very interesting demos and announcements, especially for those who are into Deep Learning. I was curious about Gmail's Smart Compose and Google Duplex besides other things, both of these being use cases of Natural Language Processing. Google Duplex got a lot of attention which was not surprising since the conversation made by Google Assistant with a real human sounded seamless and also human-like. If you are into Deep Learning, you would surely want to know at least a bit about how these are done and Google's Research blog is a good place to know more. And just in case you didn't know, ai.googleblog.com is Google's new research blog!  

We all are familiar with suggestions for word completion on phones. But why limit this feature to only word completion? Smart Compose is the new Gmail feature that helps in completing sentences by providing suggestions as we compose our emails. This too like many of the recent advancements and features, leverage neural networks. In a typical word completion task, the suggestive word is dependent on the prefix sequence of tokens/words. Depending on the previous words, the semantically closest word that could follow next is suggested. In the case of Smart Compose, in addition to the prefix sequence, the email header and also the previous email body(if present as in the case when the current mail is a response to an earlier mail) are considered as inputs to provide suggestions for sentence completion. 

<p align="center"><img src="https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s640/image2.gif"/></p>
<p align="center"><a href="https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">Credit</a></p>  

Latency, scalability and privacy are three identified challenges that should be addressed to make this feature efficient, accessible and safe. For the sentence completion suggestions to be effective, the latency should be really less so that the user doesn't notice any delays. Considering that there are more than a billion people who use Gmail, it is quite hard to even imagine how different the context of each email would be. The model should be complex and scalable enough to accommodate as many users. Shouts of privacy are heard everywhere when any kind of data is involved. And there are only a few things that demand privacy more than our emails. The models therefore should not expose the users' data in any way.  

Google's blog on Smart Compose had references to few research papers which were helpful for building the neural network architecture behind this new Gmail feature. There was also the paper *Semantics derived automatically from language corpora necessarily contain human biases* that was shared in the blog. It's a very interesting paper and it led me to a series of blogs. Even if you are no AI researcher nor an expert in Machine Learning, you must have heard about how many aspects of our lives are vastly changing by the advancements in those fields. Privacy and bias are two big concerns surrounding the usage of applications or programs that learn from data. While privacy has been a concern for quite sometime already, bias too seems to be catching up. Any program that leverages Machine Learning to solve a problem needs data as input. Depending on the problem or task at hand, this input data could be text, image, audio etc. Irrespective of what type/form the input data is, it is all generated through some human action. And that means the prejudices and biases that exist in human actions and thoughts are well part of the most important block of a learning model - its input.  

When Latanya Sweeney from Harvard University searched on Google for her own name, to her surprise the results also had an ad that was showing,"Latanya Sweeney arrested?"This led her to a research on Google search results from which she concluded that the names which are associated more with the blacks are more likely to have such search results or advertisements. She found in her research after running the search for more than 2000 real names that the names associated with blacks were up to 25% more likely to show such crimial ads. Was Google search being racist here? This was the response that came from Google - “AdWords does not conduct any racial profiling. We also have an “anti” and violence policy which states that we will not allow ads that advocate against an organisation, person or group of people. It is up to individual advertisers to decide which keywords they want to choose to trigger their ads."  


<p align="center"><img src="/assets/images/threeblackteenagers.png"/></p>
<p align="center"><a href="http://atlantablackstar.com/2016/06/10/teen-googles-three-black-teenagers-and-three-white-teenagers-to-startling-results/">Pic Credit</a></p>

In 2016, an internet user posted on Twitter, one of his experiences with Google image search. When he searched for "three white teenagers", the results he found were images of smiling, happy white teenagers. When a similar search was tried with "three black teenagers", the results were quite different. Though these results too had some normal and generic images, many of the other results were that of jailed teenagers. The post went viral soon after it was posted and that shouldn't be surprising. What was the reason behind this? Was the algorithm behind the image search programmed to do this? This would surely not be the case. Google has explained this situation too - "Our image search results are a reflection of content from across the web, including the frequency with which types of images appear and the way they're described online".  

